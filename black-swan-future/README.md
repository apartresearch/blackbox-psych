# Black Swan Sensitivity?
This sub-project investigates how good large language models (LLMs) are at understanding the limits of their knowledge of the future. I.e. does the model confidently answer knowledge of the future?

The project has three concrete aims: 
1. Evaluate the within-dataset performance of the models?
2. Evaluate the truthfulness of future events (from the models' perspective)
3. Investigate helpful prompts for making the model less sensitive?